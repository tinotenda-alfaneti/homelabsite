posts:
    - id: novnc-java-swing-apps
      title: Deploying Java Swing Applications with noVNC
      date: 2025-12-15T00:00:00Z
      category: Cloud Native
      summary: A comprehensive guide to deploying legacy Java Swing desktop applications in Kubernetes using noVNC, enabling browser-based access to GUI applications.
      content: |
        ## What is noVNC?

        noVNC is a browser-based VNC (Virtual Network Computing) client that allows you to access graphical desktop applications through a web browser without requiring any client-side software installation. It uses WebSockets to provide VNC connectivity directly in the browser.

        ### Key Features

        - Zero Client Installation - Users only need a modern web browser
        - Cross-Platform - Works on Windows, Mac, Linux, mobile devices
        - HTML5 Canvas - Renders the remote desktop using standard web technologies
        - WebSocket Protocol - Efficient real-time bidirectional communication
        - Secure - Can be served over HTTPS/WSS for encrypted connections

        ## Why Use noVNC for Java Swing Applications?

        Traditional Java Swing applications are desktop GUI applications that require:

        1. Java Runtime Environment (JRE) installed on the client
        2. Direct access to the application JAR file
        3. Local execution on the user's machine

        By using noVNC, we can:

        - Deploy Java GUI apps to Kubernetes like web applications
        - Access via web browser - No Java installation required on client
        - Centralized execution - App runs on server, reducing client requirements
        - Remote access - Users can access from anywhere
        - Consistent environment - Same Java version and dependencies for all users

        ## Architecture Overview

        The architecture consists of several layers working together:

        ```
        Web Browser (Any Device)
             |
             | HTTP/WebSocket (Port 6080)
             v
        noVNC (WebSockify)
             |
             | VNC Protocol (Port 5900)
             v
        x11vnc (VNC Server)
             |
             v
        Xvfb (Virtual Frame Buffer)
             |
             v
        Java Swing Application
        ```

        Each component serves a specific purpose:

        - **Web Browser**: User interface, no installation required
        - **noVNC/WebSockify**: Converts WebSocket connections to VNC protocol
        - **x11vnc**: VNC server that shares the X display
        - **Xvfb**: Virtual X server providing headless X11 display
        - **Java Swing App**: Your application running normally

        ## How Our Implementation Works

        ### 1. Virtual Display (Xvfb)

        ```bash
        Xvfb :99 -screen 0 1280x720x24 &
        ```

        Xvfb (X Virtual Framebuffer) creates a virtual display :99 with resolution 1280x720 and 24-bit color. It runs in memory without requiring a physical monitor, allowing GUI applications to run in headless environments like containers.

        ### 2. Window Manager (Fluxbox)

        ```bash
        fluxbox &
        ```

        Fluxbox is a lightweight X11 window manager that handles window decorations, focus, and resizing. It's essential for proper Java Swing window behavior and uses minimal resources compared to full desktop environments.

        ### 3. VNC Server (x11vnc)

        ```bash
        x11vnc -display :99 -nopw -listen 0.0.0.0 -xkb -forever -shared &
        ```

        The x11vnc server captures the virtual display and makes it available via VNC protocol:

        - `-display :99` - Connect to virtual display
        - `-nopw` - No password (suitable for containerized deployment)
        - `-listen 0.0.0.0` - Accept connections from any IP
        - `-xkb` - Enable keyboard extensions
        - `-forever` - Keep running after client disconnects
        - `-shared` - Allow multiple simultaneous connections

        ### 4. WebSocket Proxy (websockify)

        ```bash
        websockify --web=/usr/share/novnc/ 6080 localhost:5900 &
        ```

        Websockify converts WebSocket connections on port 6080 to VNC protocol on port 5900. It also serves the noVNC HTML and JavaScript files, acting as a bridge between the browser and VNC server.

        ### 5. Java Application

        ```bash
        export DISPLAY=:99
        java -cp "chess-game.jar:lib/*" com.chess.jChess &
        ```

        The Java application runs normally, with the DISPLAY environment variable pointing to the virtual display. The application renders to Xvfb, which is captured by x11vnc.

        ### 6. Window Automation (xdotool)

        ```bash
        xdotool search --name "Atarist Chess" | head -1
        xdotool windowsize $WINDOW_ID 1200 900
        xdotool windowmove $WINDOW_ID 40 40
        xdotool windowactivate $WINDOW_ID
        ```

        xdotool automatically finds the Java window, resizes it to optimal viewing size, positions it prominently on screen, and brings it to the foreground.

        ## Setting Up noVNC for Your Own Java Application

        ### Step 1: Create Dockerfile

        ```dockerfile
        FROM ubuntu:22.04

        ENV DEBIAN_FRONTEND=noninteractive \
            DISPLAY=:99 \
            VNC_RESOLUTION=1280x720

        # Install dependencies
        RUN apt-get update && apt-get install -y \
            openjdk-17-jre \
            x11vnc \
            xvfb \
            fluxbox \
            novnc \
            websockify \
            xdotool \
            && apt-get clean

        # Copy your Java application
        COPY your-app.jar /app/
        COPY lib /app/lib

        # Startup script
        RUN echo '#!/bin/bash\n\
        Xvfb :99 -screen 0 ${VNC_RESOLUTION}x24 &\n\
        sleep 2\n\
        fluxbox &\n\
        sleep 1\n\
        x11vnc -display :99 -nopw -listen 0.0.0.0 -xkb -forever -shared &\n\
        sleep 1\n\
        websockify --web=/usr/share/novnc/ 6080 localhost:5900 &\n\
        sleep 2\n\
        export DISPLAY=:99\n\
        cd /app\n\
        java -jar your-app.jar\n\
        ' > /start.sh && chmod +x /start.sh

        EXPOSE 6080 5900

        CMD ["/start.sh"]
        ```

        ### Step 2: Build and Run

        ```bash
        # Build image
        docker build -t my-java-gui-app .

        # Run container
        docker run -d -p 6080:6080 -p 5900:5900 my-java-gui-app

        # Access via browser
        open http://localhost:6080/vnc.html
        ```

        ### Step 3: Access the Application

        The noVNC web interface will appear with a "Connect" button. Click it to access your Java GUI application directly in your browser.

        ## Configuration Options

        ### Display Resolution

        Adjust the VNC_RESOLUTION environment variable for different screen sizes:

        ```dockerfile
        ENV VNC_RESOLUTION=1920x1080  # For larger displays
        ENV VNC_RESOLUTION=1024x768   # For smaller displays
        ```

        ### VNC Password Protection

        Add password protection to x11vnc for secure access:

        ```bash
        x11vnc -display :99 -passwd mypassword -listen 0.0.0.0 -xkb -forever -shared &
        ```

        ### Multiple Users

        The `-shared` flag allows multiple users to connect simultaneously. Remove it for exclusive access:

        ```bash
        x11vnc -display :99 -nopw -listen 0.0.0.0 -xkb -forever &
        ```

        ### Performance Tuning

        Optimize performance for different network conditions:

        ```bash
        # Faster screen updates
        x11vnc -display :99 -noxdamage -nopw -listen 0.0.0.0

        # Lower quality for better performance
        x11vnc -display :99 -scale 0.75 -quality 5 -nopw
        ```

        ## Kubernetes Deployment

        ### Service Configuration

        ```yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: java-gui-app
        spec:
          ports:
            - name: novnc
              port: 80
              targetPort: 6080    # noVNC web interface
            - name: vnc
              port: 5900
              targetPort: 5900    # Direct VNC (optional)
        ```

        ### Ingress Configuration

        ```yaml
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        metadata:
          name: java-gui-app
          annotations:
            nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
            nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
        spec:
          rules:
            - host: my-app.example.com
              http:
                paths:
                  - path: /
                    pathType: Prefix
                    backend:
                      service:
                        name: java-gui-app
                        port:
                          number: 80
        ```

        ## Security Considerations

        ### Production Recommendations

        **1. Enable TLS/SSL**

        - Use HTTPS for noVNC connection
        - Configure Kubernetes Ingress with TLS certificates
        - Ensure WebSocket connections use WSS protocol

        **2. Add Authentication**

        - Implement VNC password protection
        - Use HTTP basic auth at Ingress level
        - Consider OAuth2 proxy for enterprise SSO

        **3. Network Isolation**

        - Don't expose port 5900 externally
        - Only expose port 6080 through Ingress
        - Use NetworkPolicies to restrict access

        **4. Resource Limits**

        Define resource constraints to prevent resource exhaustion:

        ```yaml
        resources:
          limits:
            cpu: 1000m
            memory: 1Gi
          requests:
            cpu: 500m
            memory: 512Mi
        ```

        ## Troubleshooting

        ### Black Screen in noVNC

        Check if Xvfb and x11vnc are running properly:

        ```bash
        # Check if Xvfb is running
        ps aux | grep Xvfb

        # Restart x11vnc with verbose output
        x11vnc -display :99 -bg -nopw -listen localhost -xkb
        ```

        ### Java Window Not Visible

        List all X windows and verify the Java process:

        ```bash
        # List all X windows
        export DISPLAY=:99
        xwininfo -root -tree

        # Check Java process
        ps aux | grep java
        ```

        ### Performance Issues

        - Reduce screen resolution to lower bandwidth requirements
        - Lower VNC quality settings
        - Use `-noxdamage` flag with x11vnc
        - Ensure adequate CPU and memory resources

        ### Connection Refused

        Verify that websockify is listening and check container logs:

        ```bash
        # Check if websockify is running
        netstat -tlnp | grep 6080

        # Check container logs
        docker logs <container-name>
        ```

        ## Alternatives to noVNC

        ### For Production Environments

        **Apache Guacamole**

        - Full remote desktop gateway
        - Supports VNC, RDP, SSH
        - Built-in authentication and user management
        - More complex setup but enterprise-ready

        **X2Go**

        - Remote desktop solution specifically for X11
        - Better performance than VNC
        - Requires client installation

        **XPRA**

        - "Screen for X11" - seamless window forwarding
        - Better performance than VNC for individual applications
        - More complex configuration

        ### For Modern Web Deployment

        Consider rewriting the GUI using modern web technologies:

        - **JavaFX** with Gluon for web deployment
        - **Vaadin** - Java framework for web UIs
        - **React/Vue + REST API** - Modern web stack with Java backend

        ## Resources

        - noVNC Official Site: https://novnc.com/
        - noVNC GitHub: https://github.com/novnc/noVNC
        - x11vnc Documentation: http://www.karlrunge.com/x11vnc/
        - Xvfb Manual: https://www.x.org/releases/X11R7.6/doc/man/man1/Xvfb.1.xhtml
        - Docker + VNC Best Practices: https://github.com/ConSol/docker-headless-vnc-container

        ## Summary

        noVNC provides an elegant solution for deploying Java Swing desktop applications as cloud-native services. While it adds complexity compared to true web applications, it enables:

        - Legacy Java GUI apps in Kubernetes
        - Zero client-side installation
        - Browser-based access from any device
        - Centralized deployment and updates

        For the Atarist Chess Game, this approach allows players to enjoy the classic Java Swing interface directly in their browser, making the game accessible to anyone with a modern web browser without requiring Java installation.

        The combination of Xvfb, x11vnc, and noVNC transforms desktop applications into web-accessible services, bridging the gap between legacy GUI applications and modern cloud-native deployment practices.
      tags:
        - java
        - vnc
        - kubernetes
        - gui
        - cloud native
    - id: kubernetes-homelab-journey
      title: Building a Kubernetes Homelab
      date: 2025-12-14T21:53:55.842Z
      category: Infrastructure
      summary: My journey from bare metal to a fully automated Kubernetes cluster with Jenkins CI/CD, Trivy security scanning, and Cloudflare integration.
      content: |
        ## The Beginning

        Every Senior Software Engineer needs hands-on infrastructure experience. That's why I built my homelab - a Kubernetes cluster running on bare metal.

        ## Architecture Overview

        My homelab runs on Kubernetes with the following components:

        - **Kubernetes Cluster**: 3-node cluster (1 control plane, 2 workers)
        - **Storage**: Local PersistentVolumes with dynamic provisioning
        - **Networking**: Flannel CNI with MetalLB for LoadBalancer services
        - **Ingress**: NGINX Ingress Controller with Cloudflare DNS
        - **CI/CD**: Jenkins with Kaniko for containerized builds
        - **Security**: Trivy for vulnerability scanning at build time
        - **Monitoring**: Prometheus + Grafana (coming soon)

        ## Deployment Pipeline

        Every service follows the same deployment pattern:

        1. **Build**: Kaniko builds container images inside Kubernetes
        2. **Scan**: Trivy scans for vulnerabilities
        3. **Push**: Images pushed to Docker Hub
        4. **Deploy**: Helm charts deploy to Kubernetes
        5. **Expose**: NGINX Ingress + Cloudflare for public access

        ## Why This Matters

        This homelab isn't just for fun - it's my learning laboratory for:

        - Kubernetes operations and troubleshooting
        - CI/CD pipeline design
        - Security best practices
        - Infrastructure as Code
        - Container orchestration at scale

        Stay tuned for more posts on my journey!
      tags:
        - kubernetes
        - jenkins
        - devops
        - automation
    - id: jenkins-kubernetes-cicd
      title: Building CI/CD Pipelines with Jenkins on Kubernetes
      date: 2024-12-05T00:00:00Z
      category: DevOps
      summary: How I set up Jenkins to build, test, and deploy applications directly in Kubernetes using Kaniko and Helm.
      content: |
        ## The Challenge

        Traditional Jenkins setups use Docker-in-Docker for builds, which has security implications. I needed a better solution for my Kubernetes homelab.

        ## The Solution: Kaniko

        Kaniko builds container images inside Kubernetes without requiring Docker daemon access. Here's how I integrated it:

        ### Pipeline Structure

        Every project follows this pattern:

        ```groovy
        pipeline {
          agent any
          stages {
            stage('Build Image') {
              steps {
                sh 'kubectl apply -f ci/kubernetes/kaniko.yaml'
              }
            }
            stage('Security Scan') {
              steps {
                sh 'kubectl apply -f ci/kubernetes/trivy.yaml'
              }
            }
            stage('Deploy') {
              steps {
                sh 'helm upgrade --install app ./charts/app'
              }
            }
          }
        }
        ```

        ## Benefits

        - **Security**: No privileged containers
        - **Consistency**: Same build environment every time
        - **Scalability**: Kubernetes handles resource allocation
        - **Speed**: Parallel builds when needed

        ## Lessons Learned

        1. Always use image caching to speed up builds
        2. Keep Helm values separate for different environments
        3. Use Trivy to catch vulnerabilities early
        4. Automate everything - manual deployments are error-prone
      tags:
        - jenkins
        - kubernetes
        - cicd
        - kaniko
    - id: golang-web-services
      title: Why I Choose Go for My Homelab Services
      date: 2024-11-28T00:00:00Z
      category: Programming
      summary: Exploring the benefits of Go for building lightweight, performant web services in a resource-constrained homelab environment.
      content: |
        ## The Language Decision

        When building services for my homelab, I chose Go. Here's why:

        ## 1. Performance

        Go compiles to native binaries that are incredibly fast and use minimal resources. Perfect for a homelab where every MB of RAM counts.

        ## 2. Single Binary Deployment

        No dependencies, no runtime. Just copy the binary and run. This makes containerization trivial:

        ```dockerfile
        FROM scratch
        COPY app /app
        ENTRYPOINT ["/app"]
        ```

        ## 3. Built-in Concurrency

        Goroutines make it easy to handle multiple requests efficiently. My services can handle hundreds of concurrent users without breaking a sweat.

        ## 4. Standard Library

        The `net/http` package is production-ready out of the box. No need for heavy frameworks.

        ## Real Examples

        - **AvidLearner**: Go backend serving React frontend
        - **Ebook Reader**: Pure Go with embedded static files
        - **LabMan**: CLI tool with SSH session management
        - **Rebalancer Operator**: Kubernetes controller-runtime

        ## The Results

        My Go services typically use:
        - 10-20MB RAM
        - <5% CPU under load
        - <10MB container images (with distroless)
        - <100ms response times

        This efficiency lets me run more services on the same hardware.
      tags:
        - golang
        - web development
        - performance
    - id: learning-kubernetes-operators
      title: Building My First Kubernetes Operator
      date: 2024-11-15T00:00:00Z
      category: Kubernetes
      summary: The journey of creating a Kubernetes operator for pod rebalancing - from concept to implementation.
      content: |
        ## The Problem

        I noticed my Kubernetes cluster would sometimes have unbalanced pod distribution across nodes. This led to resource hotspots and inefficiency.

        ## The Solution: Rebalancer Operator

        I built a Kubernetes operator that:

        1. Watches pod and node resource usage
        2. Calculates optimal pod distribution
        3. Safely migrates pods to balance the cluster
        4. Respects PodDisruptionBudgets and node affinities

        ## Architecture

        Built with:
        - **controller-runtime**: Kubernetes controller framework
        - **Custom Resources**: RebalancingPolicy CRD
        - **Webhooks**: Admission validation
        - **Metrics**: Prometheus integration

        ## What I Learned

        ### 1. Kubernetes Internals
        Understanding controllers, reconciliation loops, and the API server was crucial.

        ### 2. Concurrency Patterns
        Managing multiple workers and handling race conditions required careful design.

        ### 3. Testing Strategies
        Testing operators requires mocking the Kubernetes API - I used envtest.

        ### 4. Operational Concerns
        - Leader election for high availability
        - Graceful degradation
        - Comprehensive logging and metrics

        ## Current Status

        The operator is in active development. Next steps:

        - Add cost-aware rebalancing
        - Implement dry-run mode
        - Create comprehensive dashboards
        - Write operator hub integration

        This project taught me more about Kubernetes than any certification could.
      tags:
        - kubernetes
        - operators
        - golang
        - controllers
    - id: homelab-network-setup
      title: Network Design for a Production Homelab
      date: 2024-11-01T00:00:00Z
      category: Networking
      summary: How I designed and secured my homelab network with VLANs, firewalls, and Cloudflare tunnels.
      content: |
        ## Network Architecture

        A proper homelab needs proper networking. Here's my setup:

        ### VLANs

        - **VLAN 10**: Management (switches, APs, IPMI)
        - **VLAN 20**: Kubernetes cluster
        - **VLAN 30**: Services (databases, storage)
        - **VLAN 40**: IoT and untrusted devices
        - **VLAN 50**: User devices

        ### Firewall Rules

        - Default deny all inter-VLAN traffic
        - Allow specific services (DNS, NTP)
        - Kubernetes VLAN can reach Services VLAN
        - IoT devices completely isolated

        ### External Access

        Instead of port forwarding, I use:

        1. **Cloudflare Tunnels**: Secure inbound connections
        2. **Zero Trust Access**: Authentication layer
        3. **NGINX Ingress**: Internal routing
        4. **Let's Encrypt**: Automatic SSL certificates

        ## Security Layers

        1. **Perimeter**: Cloudflare protects against DDoS
        2. **Network**: Firewall rules segment traffic
        3. **Application**: NGINX ingress with rate limiting
        4. **Container**: Network policies in Kubernetes

        ## Monitoring

        - Uptime Kuma for service availability
        - Prometheus for metrics (coming soon)
        - Loki for log aggregation (planned)

        ## Lessons Learned

        - Start with security, not as an afterthought
        - Document everything - you'll forget why you did something
        - Test firewall rules before applying
        - Cloudflare tunnels are a game-changer for homelabs

        This network design gives me production-like experience without exposing my home network to the internet.
      tags:
        - networking
        - security
        - cloudflare
    - id: homelab-architecture-deep-dive
      title: "Deep Dive: My Homelab Architecture and Service Interactions"
      date: 2025-12-14T16:00:00Z
      category: Infrastructure
      summary: A comprehensive breakdown of my homelab's architecture - from the bare metal layer through Kubernetes orchestration to external access via Cloudflare tunnels.
      content: |
        ## Overview

        My homelab is an infrastructure running entirely on Kubernetes, with carefully designed layers for networking, security, monitoring, and service delivery. Here's how everything fits together.

        ## Architecture Layers

        ### 1. Infrastructure Foundation

        **Bare Metal Cluster:**
        - 3-node Kubernetes cluster (1 control plane, 2 workers)
        - Local storage with dynamic PersistentVolume provisioning
        - Flannel CNI for pod networking
        - MetalLB for LoadBalancer service types

        **Network Architecture:**
        - Segregated VLANs for management, services, and user traffic
        - Firewall rules controlling inter-VLAN communication
        - Internal DNS resolution for service discovery
        - External DNS via Cloudflare

        ### 2. Kubernetes Control Plane

        The brain of the operation - managing workload orchestration, scheduling, and service discovery across all nodes.

        **Key Components:**
        - etcd for distributed configuration
        - kube-apiserver as the central management interface
        - kube-scheduler for intelligent pod placement
        - kube-controller-manager for cluster state reconciliation

        ### 3. Service Mesh and Networking

        **Ingress Layer:**
        - NGINX Ingress Controller as the gateway
        - TLS termination with Let's Encrypt certificates
        - Path-based routing to backend services
        - Rate limiting and request filtering

        **Service Discovery:**
        - CoreDNS for internal service resolution
        - Kubernetes Services with ClusterIP/LoadBalancer
        - Headless services for StatefulSets

        ### 4. Application Services

        **Development & Learning:**
        - **AvidLearner**: Go learning platform with React frontend
        - **LabMan CLI**: Remote homelab session management via SSH
        - **Rebalancer Operator**: Custom Kubernetes operator for pod optimization

        **Media & Content:**
        - **Navidrome**: Self-hosted music streaming (Subsonic API)
        - **Ebook Reader**: Digital library with progress tracking
        - **Poetry Blog**: Content publishing platform

        **DevOps & Automation:**
        - **Jenkins**: CI/CD orchestration with Kaniko builds
        - **Trivy**: Container image security scanning
        - **Uptime Kuma**: Service health monitoring

        **Observability Stack:**
        - **Prometheus**: Metrics collection and alerting
        - **Grafana**: Visualization and dashboards
        - **Loki**: Log aggregation (planned)

        ### 5. CI/CD Pipeline Flow

        The deployment pipeline is fully automated:

        1. **Code Push**: Developer pushes to Git repository
        2. **Jenkins Trigger**: Webhook triggers Jenkins pipeline
        3. **Build Phase**: Kaniko builds container image inside Kubernetes (no Docker daemon needed)
        4. **Security Scan**: Trivy scans image for CVEs and misconfigurations
        5. **Registry Push**: Clean images pushed to Docker Hub
        6. **Helm Deploy**: Helm charts deploy updated services to Kubernetes
        7. **Ingress Update**: NGINX Ingress routes traffic to new pods
        8. **Health Check**: Kubernetes readiness probes verify deployment

        ### 6. External Access Architecture

        **Cloudflare Integration:**
        - Cloudflare Tunnels eliminate port forwarding
        - Zero Trust Access for authentication
        - DDoS protection at the edge
        - Automatic SSL/TLS certificates

        **Traffic Flow:**
        ```
        Internet → Cloudflare Edge → Cloudflare Tunnel → 
        NGINX Ingress → Kubernetes Service → Application Pod
        ```

        ### 7. Storage Architecture

        **Persistent Storage:**
        - Local PersistentVolumes on each node
        - StorageClass for dynamic provisioning
        - StatefulSets for stateful applications
        - Volume snapshots for backup

        **Data Flow:**
        - Databases use local SSDs for low latency
        - Media files on larger HDDs
        - Configuration in ConfigMaps and Secrets
        - Logs aggregated to centralized storage

        ### 8. Security Model

        **Defense in Depth:**

        **Layer 1 - Perimeter:**
        - Cloudflare WAF blocks malicious traffic
        - Rate limiting prevents abuse
        - Bot protection

        **Layer 2 - Network:**
        - VLAN segmentation isolates traffic
        - Firewall rules enforce least privilege
        - Network policies in Kubernetes

        **Layer 3 - Application:**
        - NGINX Ingress with authentication
        - TLS encryption for all traffic
        - Application-level authorization

        **Layer 4 - Container:**
        - Trivy scanning prevents vulnerable images
        - Non-root containers
        - Read-only root filesystems
        - Resource limits prevent DoS

        ### 9. Monitoring and Observability

        **Metrics Collection:**
        - Prometheus scrapes metrics from all services
        - Node exporters on each Kubernetes node
        - Application metrics via client libraries
        - Custom metrics from Kubernetes API

        **Visualization:**
        - Grafana dashboards for real-time insights
        - Uptime Kuma for service availability
        - Alert manager for critical events

        **Logging:**
        - Container logs aggregated by Kubernetes
        - Planned Loki deployment for log queries
        - Audit logs for security events

        ### 10. Service Interactions

        **Typical Request Flow (External Service):**

        1. User requests `https://avidlearner.atarnet.org`
        2. DNS resolves to Cloudflare edge server
        3. Cloudflare Tunnel forwards to homelab
        4. NGINX Ingress receives request
        5. Ingress routes to AvidLearner Service
        6. Service load-balances to healthy pod
        7. Pod processes request and returns response
        8. Response flows back through the stack

        **Internal Service Communication:**

        Services communicate directly via Kubernetes DNS:
        - `http://service-name.namespace.svc.cluster.local`
        - No external egress required
        - Low latency within cluster
        - Encrypted with service mesh (future)

        ### 11. Disaster Recovery

        **Backup Strategy:**
        - Helm charts in Git (Infrastructure as Code)
        - PersistentVolume snapshots
        - Configuration in version control
        - Database backups to external storage

        **Recovery Process:**
        1. Rebuild Kubernetes cluster from scratch
        2. Apply Helm charts to recreate services
        3. Restore PersistentVolume data
        4. Verify service health and connectivity

        ## Key Design Decisions

        ### Why Kubernetes?

        - **Declarative Infrastructure**: GitOps workflow
        - **Self-Healing**: Automatic pod restarts and rescheduling
        - **Scalability**: Easy to add nodes and scale services
        - **Industry Standard**: Skills transfer to professional environments

        ### Why Kaniko for Builds?

        - **Security**: No privileged Docker daemon
        - **Kubernetes Native**: Runs as regular pods
        - **Consistency**: Same environment every build
        - **Caching**: Layer caching for faster builds

        ### Why Cloudflare Tunnels?

        - **Security**: No open ports on home network
        - **Simplicity**: No dynamic DNS management
        - **Performance**: Global edge network
        - **Protection**: Built-in DDoS mitigation

        ## Current Stats

        - **Services Running**: 10+ production services
        - **Container Images**: All custom-built via CI/CD
        - **Average Deployment Time**: ~5 minutes from code to production
        - **Uptime**: 99.5%+ for critical services
        - **Resource Usage**: ~60% cluster capacity

        ## Future Enhancements

        **Short Term:**
        - Implement service mesh (Istio/Linkerd)
        - Deploy Loki for centralized logging
        - Add distributed tracing (Jaeger)
        - Implement automated backups

        **Long Term:**
        - Multi-cluster federation
        - GitOps with ArgoCD
        - Chaos engineering experiments
        - Cost optimization with spot instances (if moving to cloud)

        ## Lessons Learned

        1. **Start Simple**: Don't over-engineer early. Add complexity as needed.
        2. **Document Everything**: Future you will thank present you.
        3. **Automate Relentlessly**: Manual processes breed errors.
        4. **Security First**: Easier to build in than bolt on later.
        5. **Monitor Everything**: Can't fix what you can't see.

        ## Conclusion

        This homelab architecture provides a production-like environment for learning, experimentation, and hosting real services. Every component serves a purpose, and the entire stack is reproducible via Infrastructure as Code.

        The beauty of this setup is that it mirrors real-world enterprise architectures - the skills and patterns I use here translate directly to professional cloud-native environments.

        Want to see the code? Most of my services and configurations are available on [GitHub](https://github.com/tinotenda-alfaneti).
      tags:
        - kubernetes
        - architecture
        - infrastructure
        - devops
        - networking
        - ci-cd
