posts:
    - id: kubernetes-homelab-journey
      title: Building a Kubernetes Homelab
      date: 2025-12-14T21:53:55.842Z
      category: Infrastructure
      summary: My journey from bare metal to a fully automated Kubernetes cluster with Jenkins CI/CD, Trivy security scanning, and Cloudflare integration.
      content: |
        ## The Beginning

        Every Senior Software Engineer needs hands-on infrastructure experience. That's why I built my homelab - a Kubernetes cluster running on bare metal.

        ## Architecture Overview

        My homelab runs on Kubernetes with the following components:

        - **Kubernetes Cluster**: 3-node cluster (1 control plane, 2 workers)
        - **Storage**: Local PersistentVolumes with dynamic provisioning
        - **Networking**: Flannel CNI with MetalLB for LoadBalancer services
        - **Ingress**: NGINX Ingress Controller with Cloudflare DNS
        - **CI/CD**: Jenkins with Kaniko for containerized builds
        - **Security**: Trivy for vulnerability scanning at build time
        - **Monitoring**: Prometheus + Grafana (coming soon)

        ## Deployment Pipeline

        Every service follows the same deployment pattern:

        1. **Build**: Kaniko builds container images inside Kubernetes
        2. **Scan**: Trivy scans for vulnerabilities
        3. **Push**: Images pushed to Docker Hub
        4. **Deploy**: Helm charts deploy to Kubernetes
        5. **Expose**: NGINX Ingress + Cloudflare for public access

        ## Why This Matters

        This homelab isn't just for fun - it's my learning laboratory for:

        - Kubernetes operations and troubleshooting
        - CI/CD pipeline design
        - Security best practices
        - Infrastructure as Code
        - Container orchestration at scale

        Stay tuned for more posts on my journey!
      tags:
        - kubernetes
        - jenkins
        - devops
        - automation
    - id: jenkins-kubernetes-cicd
      title: Building CI/CD Pipelines with Jenkins on Kubernetes
      date: 2024-12-05T00:00:00Z
      category: DevOps
      summary: How I set up Jenkins to build, test, and deploy applications directly in Kubernetes using Kaniko and Helm.
      content: |
        ## The Challenge

        Traditional Jenkins setups use Docker-in-Docker for builds, which has security implications. I needed a better solution for my Kubernetes homelab.

        ## The Solution: Kaniko

        Kaniko builds container images inside Kubernetes without requiring Docker daemon access. Here's how I integrated it:

        ### Pipeline Structure

        Every project follows this pattern:

        ```groovy
        pipeline {
          agent any
          stages {
            stage('Build Image') {
              steps {
                sh 'kubectl apply -f ci/kubernetes/kaniko.yaml'
              }
            }
            stage('Security Scan') {
              steps {
                sh 'kubectl apply -f ci/kubernetes/trivy.yaml'
              }
            }
            stage('Deploy') {
              steps {
                sh 'helm upgrade --install app ./charts/app'
              }
            }
          }
        }
        ```

        ## Benefits

        - **Security**: No privileged containers
        - **Consistency**: Same build environment every time
        - **Scalability**: Kubernetes handles resource allocation
        - **Speed**: Parallel builds when needed

        ## Lessons Learned

        1. Always use image caching to speed up builds
        2. Keep Helm values separate for different environments
        3. Use Trivy to catch vulnerabilities early
        4. Automate everything - manual deployments are error-prone
      tags:
        - jenkins
        - kubernetes
        - cicd
        - kaniko
    - id: golang-web-services
      title: Why I Choose Go for My Homelab Services
      date: 2024-11-28T00:00:00Z
      category: Programming
      summary: Exploring the benefits of Go for building lightweight, performant web services in a resource-constrained homelab environment.
      content: |
        ## The Language Decision

        When building services for my homelab, I chose Go. Here's why:

        ## 1. Performance

        Go compiles to native binaries that are incredibly fast and use minimal resources. Perfect for a homelab where every MB of RAM counts.

        ## 2. Single Binary Deployment

        No dependencies, no runtime. Just copy the binary and run. This makes containerization trivial:

        ```dockerfile
        FROM scratch
        COPY app /app
        ENTRYPOINT ["/app"]
        ```

        ## 3. Built-in Concurrency

        Goroutines make it easy to handle multiple requests efficiently. My services can handle hundreds of concurrent users without breaking a sweat.

        ## 4. Standard Library

        The `net/http` package is production-ready out of the box. No need for heavy frameworks.

        ## Real Examples

        - **AvidLearner**: Go backend serving React frontend
        - **Ebook Reader**: Pure Go with embedded static files
        - **LabMan**: CLI tool with SSH session management
        - **Rebalancer Operator**: Kubernetes controller-runtime

        ## The Results

        My Go services typically use:
        - 10-20MB RAM
        - <5% CPU under load
        - <10MB container images (with distroless)
        - <100ms response times

        This efficiency lets me run more services on the same hardware.
      tags:
        - golang
        - web development
        - performance
    - id: learning-kubernetes-operators
      title: Building My First Kubernetes Operator
      date: 2024-11-15T00:00:00Z
      category: Kubernetes
      summary: The journey of creating a Kubernetes operator for pod rebalancing - from concept to implementation.
      content: |
        ## The Problem

        I noticed my Kubernetes cluster would sometimes have unbalanced pod distribution across nodes. This led to resource hotspots and inefficiency.

        ## The Solution: Rebalancer Operator

        I built a Kubernetes operator that:

        1. Watches pod and node resource usage
        2. Calculates optimal pod distribution
        3. Safely migrates pods to balance the cluster
        4. Respects PodDisruptionBudgets and node affinities

        ## Architecture

        Built with:
        - **controller-runtime**: Kubernetes controller framework
        - **Custom Resources**: RebalancingPolicy CRD
        - **Webhooks**: Admission validation
        - **Metrics**: Prometheus integration

        ## What I Learned

        ### 1. Kubernetes Internals
        Understanding controllers, reconciliation loops, and the API server was crucial.

        ### 2. Concurrency Patterns
        Managing multiple workers and handling race conditions required careful design.

        ### 3. Testing Strategies
        Testing operators requires mocking the Kubernetes API - I used envtest.

        ### 4. Operational Concerns
        - Leader election for high availability
        - Graceful degradation
        - Comprehensive logging and metrics

        ## Current Status

        The operator is in active development. Next steps:

        - Add cost-aware rebalancing
        - Implement dry-run mode
        - Create comprehensive dashboards
        - Write operator hub integration

        This project taught me more about Kubernetes than any certification could.
      tags:
        - kubernetes
        - operators
        - golang
        - controllers
    - id: homelab-network-setup
      title: Network Design for a Production Homelab
      date: 2024-11-01T00:00:00Z
      category: Networking
      summary: How I designed and secured my homelab network with VLANs, firewalls, and Cloudflare tunnels.
      content: |
        ## Network Architecture

        A proper homelab needs proper networking. Here's my setup:

        ### VLANs

        - **VLAN 10**: Management (switches, APs, IPMI)
        - **VLAN 20**: Kubernetes cluster
        - **VLAN 30**: Services (databases, storage)
        - **VLAN 40**: IoT and untrusted devices
        - **VLAN 50**: User devices

        ### Firewall Rules

        - Default deny all inter-VLAN traffic
        - Allow specific services (DNS, NTP)
        - Kubernetes VLAN can reach Services VLAN
        - IoT devices completely isolated

        ### External Access

        Instead of port forwarding, I use:

        1. **Cloudflare Tunnels**: Secure inbound connections
        2. **Zero Trust Access**: Authentication layer
        3. **NGINX Ingress**: Internal routing
        4. **Let's Encrypt**: Automatic SSL certificates

        ## Security Layers

        1. **Perimeter**: Cloudflare protects against DDoS
        2. **Network**: Firewall rules segment traffic
        3. **Application**: NGINX ingress with rate limiting
        4. **Container**: Network policies in Kubernetes

        ## Monitoring

        - Uptime Kuma for service availability
        - Prometheus for metrics (coming soon)
        - Loki for log aggregation (planned)

        ## Lessons Learned

        - Start with security, not as an afterthought
        - Document everything - you'll forget why you did something
        - Test firewall rules before applying
        - Cloudflare tunnels are a game-changer for homelabs

        This network design gives me production-like experience without exposing my home network to the internet.
      tags:
        - networking
        - security
        - cloudflare
    - id: homelab-architecture-deep-dive
      title: "Deep Dive: My Homelab Architecture and Service Interactions"
      date: 2025-12-14T16:00:00Z
      category: Infrastructure
      summary: A comprehensive breakdown of my homelab's architecture - from the bare metal layer through Kubernetes orchestration to external access via Cloudflare tunnels.
      content: |
        ## Overview

        My homelab is an infrastructure running entirely on Kubernetes, with carefully designed layers for networking, security, monitoring, and service delivery. Here's how everything fits together.

        ## Architecture Layers

        ### 1. Infrastructure Foundation

        **Bare Metal Cluster:**
        - 3-node Kubernetes cluster (1 control plane, 2 workers)
        - Local storage with dynamic PersistentVolume provisioning
        - Flannel CNI for pod networking
        - MetalLB for LoadBalancer service types

        **Network Architecture:**
        - Segregated VLANs for management, services, and user traffic
        - Firewall rules controlling inter-VLAN communication
        - Internal DNS resolution for service discovery
        - External DNS via Cloudflare

        ### 2. Kubernetes Control Plane

        The brain of the operation - managing workload orchestration, scheduling, and service discovery across all nodes.

        **Key Components:**
        - etcd for distributed configuration
        - kube-apiserver as the central management interface
        - kube-scheduler for intelligent pod placement
        - kube-controller-manager for cluster state reconciliation

        ### 3. Service Mesh and Networking

        **Ingress Layer:**
        - NGINX Ingress Controller as the gateway
        - TLS termination with Let's Encrypt certificates
        - Path-based routing to backend services
        - Rate limiting and request filtering

        **Service Discovery:**
        - CoreDNS for internal service resolution
        - Kubernetes Services with ClusterIP/LoadBalancer
        - Headless services for StatefulSets

        ### 4. Application Services

        **Development & Learning:**
        - **AvidLearner**: Go learning platform with React frontend
        - **LabMan CLI**: Remote homelab session management via SSH
        - **Rebalancer Operator**: Custom Kubernetes operator for pod optimization

        **Media & Content:**
        - **Navidrome**: Self-hosted music streaming (Subsonic API)
        - **Ebook Reader**: Digital library with progress tracking
        - **Poetry Blog**: Content publishing platform

        **DevOps & Automation:**
        - **Jenkins**: CI/CD orchestration with Kaniko builds
        - **Trivy**: Container image security scanning
        - **Uptime Kuma**: Service health monitoring

        **Observability Stack:**
        - **Prometheus**: Metrics collection and alerting
        - **Grafana**: Visualization and dashboards
        - **Loki**: Log aggregation (planned)

        ### 5. CI/CD Pipeline Flow

        The deployment pipeline is fully automated:

        1. **Code Push**: Developer pushes to Git repository
        2. **Jenkins Trigger**: Webhook triggers Jenkins pipeline
        3. **Build Phase**: Kaniko builds container image inside Kubernetes (no Docker daemon needed)
        4. **Security Scan**: Trivy scans image for CVEs and misconfigurations
        5. **Registry Push**: Clean images pushed to Docker Hub
        6. **Helm Deploy**: Helm charts deploy updated services to Kubernetes
        7. **Ingress Update**: NGINX Ingress routes traffic to new pods
        8. **Health Check**: Kubernetes readiness probes verify deployment

        ### 6. External Access Architecture

        **Cloudflare Integration:**
        - Cloudflare Tunnels eliminate port forwarding
        - Zero Trust Access for authentication
        - DDoS protection at the edge
        - Automatic SSL/TLS certificates

        **Traffic Flow:**
        ```
        Internet → Cloudflare Edge → Cloudflare Tunnel → 
        NGINX Ingress → Kubernetes Service → Application Pod
        ```

        ### 7. Storage Architecture

        **Persistent Storage:**
        - Local PersistentVolumes on each node
        - StorageClass for dynamic provisioning
        - StatefulSets for stateful applications
        - Volume snapshots for backup

        **Data Flow:**
        - Databases use local SSDs for low latency
        - Media files on larger HDDs
        - Configuration in ConfigMaps and Secrets
        - Logs aggregated to centralized storage

        ### 8. Security Model

        **Defense in Depth:**

        **Layer 1 - Perimeter:**
        - Cloudflare WAF blocks malicious traffic
        - Rate limiting prevents abuse
        - Bot protection

        **Layer 2 - Network:**
        - VLAN segmentation isolates traffic
        - Firewall rules enforce least privilege
        - Network policies in Kubernetes

        **Layer 3 - Application:**
        - NGINX Ingress with authentication
        - TLS encryption for all traffic
        - Application-level authorization

        **Layer 4 - Container:**
        - Trivy scanning prevents vulnerable images
        - Non-root containers
        - Read-only root filesystems
        - Resource limits prevent DoS

        ### 9. Monitoring and Observability

        **Metrics Collection:**
        - Prometheus scrapes metrics from all services
        - Node exporters on each Kubernetes node
        - Application metrics via client libraries
        - Custom metrics from Kubernetes API

        **Visualization:**
        - Grafana dashboards for real-time insights
        - Uptime Kuma for service availability
        - Alert manager for critical events

        **Logging:**
        - Container logs aggregated by Kubernetes
        - Planned Loki deployment for log queries
        - Audit logs for security events

        ### 10. Service Interactions

        **Typical Request Flow (External Service):**

        1. User requests `https://avidlearner.atarnet.org`
        2. DNS resolves to Cloudflare edge server
        3. Cloudflare Tunnel forwards to homelab
        4. NGINX Ingress receives request
        5. Ingress routes to AvidLearner Service
        6. Service load-balances to healthy pod
        7. Pod processes request and returns response
        8. Response flows back through the stack

        **Internal Service Communication:**

        Services communicate directly via Kubernetes DNS:
        - `http://service-name.namespace.svc.cluster.local`
        - No external egress required
        - Low latency within cluster
        - Encrypted with service mesh (future)

        ### 11. Disaster Recovery

        **Backup Strategy:**
        - Helm charts in Git (Infrastructure as Code)
        - PersistentVolume snapshots
        - Configuration in version control
        - Database backups to external storage

        **Recovery Process:**
        1. Rebuild Kubernetes cluster from scratch
        2. Apply Helm charts to recreate services
        3. Restore PersistentVolume data
        4. Verify service health and connectivity

        ## Key Design Decisions

        ### Why Kubernetes?

        - **Declarative Infrastructure**: GitOps workflow
        - **Self-Healing**: Automatic pod restarts and rescheduling
        - **Scalability**: Easy to add nodes and scale services
        - **Industry Standard**: Skills transfer to professional environments

        ### Why Kaniko for Builds?

        - **Security**: No privileged Docker daemon
        - **Kubernetes Native**: Runs as regular pods
        - **Consistency**: Same environment every build
        - **Caching**: Layer caching for faster builds

        ### Why Cloudflare Tunnels?

        - **Security**: No open ports on home network
        - **Simplicity**: No dynamic DNS management
        - **Performance**: Global edge network
        - **Protection**: Built-in DDoS mitigation

        ## Current Stats

        - **Services Running**: 10+ production services
        - **Container Images**: All custom-built via CI/CD
        - **Average Deployment Time**: ~5 minutes from code to production
        - **Uptime**: 99.5%+ for critical services
        - **Resource Usage**: ~60% cluster capacity

        ## Future Enhancements

        **Short Term:**
        - Implement service mesh (Istio/Linkerd)
        - Deploy Loki for centralized logging
        - Add distributed tracing (Jaeger)
        - Implement automated backups

        **Long Term:**
        - Multi-cluster federation
        - GitOps with ArgoCD
        - Chaos engineering experiments
        - Cost optimization with spot instances (if moving to cloud)

        ## Lessons Learned

        1. **Start Simple**: Don't over-engineer early. Add complexity as needed.
        2. **Document Everything**: Future you will thank present you.
        3. **Automate Relentlessly**: Manual processes breed errors.
        4. **Security First**: Easier to build in than bolt on later.
        5. **Monitor Everything**: Can't fix what you can't see.

        ## Conclusion

        This homelab architecture provides a production-like environment for learning, experimentation, and hosting real services. Every component serves a purpose, and the entire stack is reproducible via Infrastructure as Code.

        The beauty of this setup is that it mirrors real-world enterprise architectures - the skills and patterns I use here translate directly to professional cloud-native environments.

        Want to see the code? Most of my services and configurations are available on [GitHub](https://github.com/tinotenda-alfaneti).
      tags:
        - kubernetes
        - architecture
        - infrastructure
        - devops
        - networking
        - ci-cd
